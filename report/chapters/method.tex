\chapter{Method}
\label{sec:method}



\section{Overview}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/system_overview.pdf}
    \caption{schematic overview of the whole system}
    \label{fig:overview}
\end{figure}


The visensor provides a stream of frames, each of which consists of a stereo
pair of grayscale intensity data \footnote{Grayscale instead of full-color
images are used, because the information gain from colors is offset by the loss
of resolution. However, the approach described here would work similarly
with colored images.}.  A semiglobal stereo matching core developed in
\cite{honegger2014sgmcore} running on the FPGA processes these and produces a
disparity image which assigns every pixel the disparity between the two
cameras. The FPGA also provides a rectified camera image.

This pair of intensity and disparity images is conceptually equivalent to a
three dimensional point-cloud, as we can calculate the distance to the camera
for every pixel from the disparity data. This is in turn makes it possible to
render the point-cloud from an arbitrary perspective, allowing us to look at an
image as if it was recorded from a different angle.

To estimate the ego-motion between two frames we can thus look for a
perspective where the re-rendered image looks the same as the previous frame.
The movement of the virtual camera will then correspond to the actual, physical
movement of the sensor.

The problem is formulized as a minimization problem, by subtracting the
intensities from the previous frame with the intensities of the current frame
sampled at the warped pixel locations. This way, a photometric error is
calculated, measuring the similarity of the warped current frame with the
previous one.

Note that this approach assumes Lambertian reflectance and constant
illumination (also known as photo-consitency), which implies that points should
have the same brightness, regardless of viewing angle.


\section{Warping Pipeline}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/warp_pipeline.pdf}
    \caption{the full warping pipeline (pink pixels are not sampled by any of the warped points)}
    \label{fig:warp_pipeline}
\end{figure}

\begin{figure}
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width = \textwidth]{images/coordinate_system_camera.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \includegraphics[width = \textwidth]{images/coordinate_systems_3d.pdf}
    \end{minipage}
    \caption{The coordinate systems used in this work.}
    \label{fig:coordinate_systems}
\end{figure}

This section closely follows \cite{omaridenseodometry}.

Using an inverse projection derived from the standard pinhole camera model, a
point $ \vec{x} $ in the camera image plane is be back-projected into a point
$ \vec{p} \in \mathbb{R}^3 $ written using homogeneous coordinates:

\begin{equation}
    \label{eq:backprojection}
    \vec{p} = \pi^{-1}(\vec{x}, D(\vec{x})) := \frac{b}{D(x)}
    \begin{bmatrix}
        x_u - c_u \\
        x_v - c_v \\
        f \\
        1
    \end{bmatrix}
\end{equation}

where $b$ is the stereo baseline, $f$ the focal length and $\vec{c}$ the principal point of the camera.

This point $\vec{p}$ can now be moved into a new camera frame by translating
and rotating it, by writing $\vec{T}$ as a $4 \times 4$ homogeneous
transformation matrix:

[TODO: soll ich das explizit als $f(\vec{T})\vec{p}$ schreiben?]

\begin{equation}
    \vec{p'} = \mat{T} \vec{p}
\end{equation}

A point in 3D space can be projected back onto the (now moved) camera image plane:

\begin{equation}
    \label{eq:projection}
    \vec{x'} = \pi(\vec{p'}) := \frac{f}{p'_z}
    \begin{bmatrix}
        p'_x \\
        p'_y \\
    \end{bmatrix}
    + \vec{c}
\end{equation}

This whole warping operator is summarized in a warping operator $\tau$:

\begin{equation}
    \vec{x'} = \tau(\vec{x}, D(\vec{x}), \vec{T}) := \pi( \mat{T} \cdot \pi^{-1} (\vec{x}, D(\vec{x})))
\end{equation}



\section{Minimization}

Using $\tau$, the error between the previous frame and the warped current frame is be defined as:

\begin{equation}
    \label{eq:error}
    e(\vec{x}, \vec{T}) := I_p(\tau(\vec{x}, \vec{T})) - I_c(\vec{x})
\end{equation}


To estimate the motion between two frames, this photometric error term should
be minimized for every pixel:

\begin{equation}
    \vec{\hat{T}} = \operatornamewithlimits{argmin}_{\vec{T}} \sum_{\vec{x} \in I_p} e(\vec{x}, \vec{T})^2
\end{equation}


This equation can be solved for the estimated motion by applying standard
optimization techniques, for example Gauss-Newton:

\begin{equation}
    \mat{J}^T \mat{J} \Delta \vec{T} = - \mat{J}^T \vec{e(\vec{T})}
\end{equation}

Here, $\mat{J} \in \mathbb{R}^{N \times 6}$ are the stacked Jacobian matrices
and $\vec{e(\vec{T})} \in \mathbb{R}^N$ the vector of the error terms of all
$N$ pixels.  This equation is iteratively solved for the increment $\Delta
\vec{T}$ of the motion estimation after recalculating the error term and
Jacobians from the new estimation.

The Jacobian is derived by applying the chain rule to the photometric error term \ref{eq:error}.
For a single pixel, we get a $1 \times 6$ Jacobian:

\begin{equation}
    \mat{J} := \mat{J_I} \mat{J_{\pi}} \mat{J_T}
\end{equation}

where $\mat{J_I} \in \mathbb{R}^{1 \times 2}$ is the image derivative of the warped previous frame and is
approximated using the image's gradient:

\begin{equation}
    \mat{J_I} := \frac{\partial I_p(\vec{x})} {\partial \vec{x}} \bigg|_{\vec{x} = \tau(\vec{x}, \vec{T})}
    \approx
    \begin{bmatrix}
        \nabla{}_x I_p & \nabla{}_y I_p
    \end{bmatrix}
\end{equation}

The term $\mat{J_{\pi}}$ is the $2 \times 3$ Jacobian of the projection
function \ref{eq:projection}, evaluated at the warped 3D point:

\begin{equation}
    \mat{J_{\pi}} := \frac{\partial \pi(\vec{p})}{\partial \vec{p}}
    \bigg|_{\vec{p} = \vec{T}\cdot\pi^{-1}(\vec{x},D(\vec{x}))}
    =
    \begin{bmatrix}
        \nicefrac{f}{\vec{p_z}} & 0 & -f \nicefrac{\vec{p_x}}{\vec{p_z}^2} \\
        0 & \nicefrac{f}{\vec{p_z}} & -f \nicefrac{\vec{p_y}}{\vec{p_z}^2}
    \end{bmatrix}
\end{equation}

$\mat{J_T}$ is the $3 \times 6$ Jacobian of the transformation operator $T$ and
the most costly term to compute:

\begin{equation}
    \mat{J_T} := \frac{\partial (\vec{T} \vec{p})}{\partial \vec{T}}
    \bigg|_{\vec{p} = \pi^{-1}(\vec{x}, D(\vec{x}))}
\end{equation}
