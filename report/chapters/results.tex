\chapter{Results}
\label{sec:results}



\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/traj_aslam_downscaled.pdf}
    \caption{qualitative comparison of photometric odometry to ASLAM. The red
    trajectory was computed using full image resolution, while the orange one
    only uses $\nicefrac{1}{16}^{th}$ of the available pixels by aborting early in
the image pyramid, thus greatly speeding up runtime performance at negligible
loss of accuracy.}
    \label{fig:trajectory}
\end{figure}

\section{Qualitative Assessment}
\label{sec:results_qualitative}

To assess the quality of photometric approach to visual odometry, an indoor
circular trajectory was processed offline with OpenCV's SGM to ensure
high-quality disparity data.  It was also run trough the very accurate ASLAM
algorithm \cite{leutenegger2013keyframe} to provide something akin to
ground-truth.

After a traveled distance of about \unit[12]{m} a drift of about \unit[20]{cm}
has been accumulated.

The trajectory was recorded in an office environment, using a vi-sensor as
described in section~\ref{sec:motivation} with a baseline of \unit[11]{cm} and a
\unit[112]{\degree} diagonal field of view, providing a resolution of $752
\times 480$ pixels. To ensure photoconsistency, the shutter speed was fixed.
The image gradient threshold was tuned to use at least \unit[25]{\%} of all
pixels. The lowest resolution used in the image pyramid was $94 \times 60$
pixels, downscaling the orignial image three times. No error weighting was
used, as the trajectory did contain few occlusions. The code was compiled for
maximum performance, using \texttt{-O3 -mfpu=neon -mfloat-abi=softfp
-ffast-math -fno-finite-math-only}.

Movement was restricted to a slow walking speed and was recorded with 30 FPS.
On faster trajectories with larger movement between frames, the Gauss-Newton
optimization diverges in badly lit or sparse parts with few image gradients,
resulting in a completely wrong trajectory.

An important finding, which was already noted in \cite{comport2007odometry}, is
that processing the camera images in their full resolution is not a necessity.
As can be seen in figure~\ref{fig:trajectory}, even using only a twice
downscaled image still results in precise tracking while drastically improving
performance.

Downscaling usually implies filtering and doing so alters the 3D structure. For
this reason, \cite{comport2007odometry} does not downscale the disparity values and
samples them at the full resolution. Though when matching pose on the camera plane
instead of in 3D space, downscaling the disparity values works as well.

It might be worth investigating how much of an effect on performance and
quality downscaling the disparity images has. Disparity values are only read at
integer coordinates and downscaling them might not be worth the runtime
penalty.

\subsection{Keeping Pixel Numbers Stable}

The optimizations described in section~\ref{sec:optimizations} can
substantially reduce the pixel count, so much so that there are not enough for
stable performance. Especially filtering pixels based on their image gradient
as explained in section~\ref{sec:gradient_filtering} requires a well-chosen
threshold, as image gradients depend on the scene. \cite{comport2007odometry}
computes a histogram to select the $N$ best pixels. An easier approach is to
simply restart the current iteration with a lower threshold when the number of
pixels gets too low and increasing it after a step with enough pixels,
therefore implementing a proportional controller as described in
\cite{omaridenseodometry}. The same problem also applies to other optimization
parameters such as the size of the image pyramid.

\section{Timing}
\label{sec:timing}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/timing.pdf}
    \caption{perfomance breakdown using early abort of the image pyramid}
    \label{fig:timing}
\end{figure}

The different parts of the algorithm have been timed by counting CPU cycles
while running the full photometric odometry on a single 667 MHz Cortex A9 ARM
core on the vi-sensor while moving around slowly. The time for the SGM is not
shown in figure~\ref{fig:timing}, as this part is running on the FPGA with the
full 30 FPS provided by the cameras.

Using early abort in the image pyramid we can achieve an average run-time
performance of about 5 Hz.
This value not only depends on the algorithm's parameters such as max. pyramid
levels, but also on the scene (more 'dense' environments which provide more
structure converge faster) and on the movement speed. The bigger the steps, the
more iterations are required for convergence, which is counterproductive as a
longer running time implies an even bigger step while moving. This could
potentially be addressed by incorporating integrated acceleration values of an
IMU.

Another important point is that at least half the time is spent on building the
image pyramid, warping pixels and computing the Jacobians. These parts are
highly parallelizable because every pixel is completely independent of each
other. This offers great potential for further optimizations: Moving the
construction of image pyramids to the FPGA is simple. Warping and the
calculation of Jacobians less so, because these steps involve a lot of
floating-point arithmetic, but they would still greatly profit from another CPU
core or usage of vector instruction (such as the ARM NEON instruction set).
