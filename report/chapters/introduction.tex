\chapter{Introduction}
\label{sec:introduction}

\section{Motivation}
\label{sec:motivation}

Robots are generally constrained in their computational power and energy usage.
A powerful method is to offload computation onto an FPGA, which is usually
a much more efficient than a general purpose processor.  However, programming
an FPGA is not straightforward and integtrating it with code running on a CPU
can be tricky.

In this work, a novel example of such an integration is provided by running a
semi-global stereo matching \cite{hirschmuller2005sgm} core developed in
\cite{honegger2014sgmcore} on the FPGA and using its output for a photometric
visual odometry algorithm \cite{comport2007odometry} running on the CPU.

This approach of photometric odometry does not track a sparse set of features,
as is usuallty done in visual odometry, but instead warps the full image to
find a perspective where the warped image matches the previous frame.

Photometric odometry is very robust against various perturbations, such as
blur, occlusions or changes in lighting \cite{comport2011asymmetric}. It is
also more accurate than feature-based methods, as it incorporates every pixel
instead of throwing out most of the data. This also increases computational
load however. Visual odoemtry in general requires a well structured
environment, as for example homogenous walls are basically impossible to track.

This "dense" approach is well suited for offloading to an FPGA, as most parts
are highly parallelizable. Note tough, that this is not the most efficient way
to do embedded odometry, as a lot more data has to be processed. The main goal
was to explore how an FPGA and a general purpose processor can be integrated on
an embedded device and to ascertain the potential for further optimizations by
transfering more parts to the FPGA.

A visual-inertial sensor developed by the ASL \cite{nikolic2014synchronized} is
used which features a Xilinx Zynq 7020 SoC consisting of a dual-core ARM Cortex
A9 and an ARTIX-7 FPGA. The sensor has a wide-angle stereo camera with a
resolution of $752 \times 480$ pixels with syncronized global shutters as well
as a high-precision inertial measurement unit, which was not used here. This
vi-sensor is not only small and lightweight ($133 \times 57$mm, \unit[130]{g}),
it is also power-efficient, consuming around \unit[5]{W}.



\section{Related Work}
\label{sec:related_work}

Photometric odometry as initially developed by Comport et al. in
\cite{comport2007odometry} has been implemented in \cite{omaridenseodometry} to
use data from the SGM core but running on a powerful PC instead.

In \cite{marcin2014odometry}, feature-based odometry running on the vi-sensor
is developed, which uses the FPGA for corner detection. While very similar in
result and method, a completely different and much more 'sparse' algorithm is
used. A real-time perfomance of 10 Hz is achieved by using vector instructions
and fusing inertial measurements.  A similiar implementation
\cite{goldberg2011stereo} uses a Texas Instrument SoC, developed for
smartphones.

Other approaches to photometric odoemtry rely on different means to get depth
data, such as an RGB-D sensor \cite{kerl2013robust}. Usually, active systems
are used which increase power consumption, are either expensive (Laser), low
resolution (time of flight) or work only in constrained scenery (structured
light). Howevery, RGB-D sensors can be more accurate, cover longer ranges and
consume less computing power.
